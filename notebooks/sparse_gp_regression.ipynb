{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook was used as the starting point: https://github.com/SheffieldML/notebook/blob/master/GPy/sparse_gp_regression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import GPy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from GPy.core.parameterization.variational import NormalPosterior\n",
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_covariance_matrix(cov_matrix):\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    im = ax.imshow(cov_matrix, interpolation='none')\n",
    "    fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Function\n",
    "\n",
    "Now we'll sample a Gaussian process regression problem directly from a Gaussian process prior. We'll use an exponentiated quadratic covariance function with a lengthscale and variance of 1 and sample 50 equally spaced points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "noise_var = 0.05\n",
    "\n",
    "X = np.linspace(0,10,50)[:,None]\n",
    "X_new = np.linspace(10,15,50)[:,None]\n",
    "k = GPy.kern.RBF(1)\n",
    "y = np.random.multivariate_normal(np.zeros(N),k.K(X)+np.eye(N)*np.sqrt(noise_var)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Gaussian Process Fit\n",
    "\n",
    "Now we use GPy to optimize the parameters of a Gaussian process given the sampled data. Here, there are no approximations, we simply fit the full Gaussian process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(X,y)\n",
    "m_full.optimize('bfgs')\n",
    "m_full.plot()\n",
    "print(m_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Z = np.hstack((np.linspace(2.5,4.,3),np.linspace(7,8.5,3)))[:,None]\n",
    "# m = GPy.models.SparseGPRegression(X,y,Z=Z)\n",
    "m = GPy.models.SparseGPRegression(X,y, num_inducing=6)\n",
    "m.likelihood.variance = noise_var\n",
    "# m.inducing_inputs.fix()\n",
    "m.rbf.variance.fix()\n",
    "m.rbf.lengthscale.fix()\n",
    "m.Z.unconstrain()\n",
    "m.optimize('bfgs')\n",
    "# m.optimize('bfgs', messages=True)\n",
    "m.plot()\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.plot_density()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = X = np.linspace(0,10,1000)[:,None]\n",
    "samples = X\n",
    "# samples = m.Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, covar = m.predict_noiseless(samples, full_cov=True)\n",
    "variances = covar.diagonal()\n",
    "variances = np.reshape(variances, (len(samples), 1))\n",
    "sparse_posterior = NormalPosterior(means=mu, variances=variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_full, covar_full = m_full.predict_noiseless(samples, full_cov=True)\n",
    "variances_full = covar_full.diagonal()\n",
    "variances_full = np.reshape(variances, (len(samples), 1))\n",
    "full_posterior = NormalPosterior(means=mu_full, variances=variances_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divergence = sparse_posterior.KL(full_posterior)\n",
    "print(divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood_sparse_model = m.log_likelihood()[0][0]\n",
    "log_likelihood_full_model = m_full.log_likelihood()\n",
    "likelihood_sparse_model = np.e**log_likelihood_sparse_model\n",
    "likelihood_full_model = np.e**log_likelihood_full_model\n",
    "print(log_likelihood_sparse_model, log_likelihood_full_model)\n",
    "print(likelihood_sparse_model, likelihood_full_model)\n",
    "print(log_likelihood_sparse_model - log_likelihood_full_model)\n",
    "print(likelihood_sparse_model - likelihood_full_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !!!!!!!!!\n",
    "^^ This time, we have enough inducing points and the fit resembles that of the GP. This is verified by the fact that the bound on the marginal likelihood is tight, which means that our variational approximation must be good (the difference between the bound and the true likelihood is the Kullback Leibler divergence between the approximation and the truth). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show covar of inducing inputs and of full gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_covariance_matrix(covar)\n",
    "plot_covariance_matrix(covar_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check posterior over inducing points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_covariance = m.posterior.covariance\n",
    "posterior_mean = m.posterior.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_covariance_matrix(posterior_covariance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
